{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbtronics/COMP551---Miniproject-3-Classification-of-Image-Data-/blob/main/Group_18_Miniproject_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MskdeYqoN1C9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import math\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccnXMmu82Kfc"
      },
      "source": [
        "# MLP\n",
        "Multilayer Perceptron Implementation\n",
        "\n",
        "Implemention will take batch or individual sample units. It can be used with SGD or mini-batch SGD or batch GD\n",
        "\n",
        "Assumes that all hidden layers have the SAME neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Wmtl4tPwduk6"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "  def __init__ (self, input_neurons, output_neurons, hidden_layer=1,\n",
        "               hidden_neurons=128,learning_rate=1e-2, activation=\"sigmoid\"):\n",
        "    # self.epochs = epochs\n",
        "    self.hidden_layer = hidden_layer\n",
        "    self.input_neurons = input_neurons\n",
        "    self.hidden_neurons = hidden_neurons\n",
        "    self.output_neurons = output_neurons\n",
        "    # self.batch_size = batch_size # decides on mini/batch stochastic gradiant descent\n",
        "    self.learning_rate = learning_rate\n",
        "    # self.epsilon = epsilon\n",
        "    self.activation = activation # decides on sigmoid, relu, leaky-relu, tanh activation\n",
        "    # no. of weights dependent on hidden_layers\n",
        "    self.weights = self.init_weights(self.hidden_layer) \n",
        "\n",
        "  def init_weights(self, hidden_layer):\n",
        "    if(hidden_layer==0):\n",
        "      w = np.random.randn(self.input_neurons, self.output_neurons) * np.sqrt(1./self.output_neurons)\n",
        "      return np.asarray([w])\n",
        "    elif (hidden_layer==1):\n",
        "      v = np.random.randn(self.input_neurons, self.hidden_neurons) * np.sqrt(1./self.hidden_neurons)\n",
        "      w = np.random.randn(self.hidden_neurons, self.output_neurons) * np.sqrt(1./self.output_neurons)\n",
        "      return np.asarray([v,w])\n",
        "    elif (hidden_layer==2):\n",
        "      v = np.random.randn(self.input_neurons, self.hidden_neurons) * np.sqrt(1./self.hidden_neurons)\n",
        "      r = np.random.randn(self.hidden_neurons, self.hidden_neurons) * np.sqrt(1./self.hidden_neurons)\n",
        "      w = np.random.randn(self.hidden_neurons, self.output_neurons) * np.sqrt(1./self.output_neurons)\n",
        "      return np.asarray([v,r,w])\n",
        "    else:\n",
        "      print(\"# of hidden layer should be <= 2\")\n",
        "      return None\n",
        "\n",
        "  def fit(self, x_train, y_train, batch_size, epochs=1e3, epsilon=1e-2,): # add batchsize here rather than in __init__\n",
        "    y_class = np.zeros((y_train.shape[0], np.max(y_train)+1))\n",
        "    for i in range(y_train.shape[0]): y_class[i,y_train[i]] = 1\n",
        "\n",
        "    batch_runs = math.ceil(y_train.shape[0]/batch_size)\n",
        "    \n",
        "    norm = np.array([np.inf])\n",
        "    t=0\n",
        "    while(np.any(norm>epsilon) and t<epochs):\n",
        "      # pdb.set_trace()\n",
        "      for batch_index in range(batch_runs): # batch_runs decides if it is mini-batch, stochastic or batch GD\n",
        "        start = batch_index*batch_size\n",
        "        end = (batch_index+1)*batch_size\n",
        "        x_train_batch = x_train[start:end] if end<=y_train.shape[0] else x_train[start:]\n",
        "        y_train_batch = y_class[start:end] if end<=y_train.shape[0] else y_class[start:] # N x C\n",
        "        y_train_label_batch = y_train[start:end] if end<=y_train.shape[0] else y_train[start:] # N\n",
        "        # print(x_train.shape)\n",
        "        # print(x_train_batch.shape)\n",
        "        yh = self.forward_pass(x_train_batch)\n",
        "        dweights = self.back_prop(yh, y_train_batch)\n",
        "        self.update_weights(np.asarray(dweights))\n",
        "      # yh = self.forward_pass(x_train)\n",
        "      # dweights = self.back_prop(yh, y_class)\n",
        "      # self.update_weights(np.asarray(dweights))\n",
        "\n",
        "      # takes into account of 0 layer\n",
        "      norm = [np.linalg.norm(dweights)] if len(dweights)==1 else [np.linalg.norm(g) for g in dweights]\n",
        "      norm = np.array(norm)\n",
        "      # norm = [np.linalg.norm(g) for g in dweights]\n",
        "      # print(\"Epoch: \", t, \" with accuracy of: \", self.eval_acc(np.argmax(yh, axis=-1), y_train))\n",
        "      # print(\"Norm: \", norm)\n",
        "      t+=1\n",
        "\n",
        "  def activation_function(self, z, derivative=False):\n",
        "    if(self.activation==\"sigmoid\"):\n",
        "      return sigmoid(z)*(1-sigmoid(z)) if derivative else sigmoid(z)\n",
        "      # if derivative: return sigmoid(z)*(1-sigmoid(z))\n",
        "      # else: return sigmoid(z)\n",
        "    elif(self.activation==\"tanh\"):\n",
        "      return 1-np.square(tanh(z)) if derivative else tanh(z)\n",
        "      # if derivative: return 1-np.square(tanh(z))\n",
        "      # else: return tanh(z)\n",
        "    elif(self.activation==\"relu\"):\n",
        "      return 1*(z>0) if derivative else relu(z)\n",
        "      # if derivative: return 1 * (z>0)\n",
        "      # else: return relu(z)\n",
        "    elif(self.activation==\"leaky-relu\"):\n",
        "      temp = 1*(z>0)\n",
        "      return (np.array(temp==0, dtype=float)*0.1+temp) if derivative else leaky_relu(z)\n",
        "      # if derivative:\n",
        "      #   return 1 if (z>0) else 0.1\n",
        "      # else: return leaky_relu(z)\n",
        "    else:\n",
        "      print(\"invalid activation function\")\n",
        "      return None\n",
        "      \n",
        "  def forward_pass(self, x_train):\n",
        "    f1 = x_train # N x D\n",
        "    self.f_params = []\n",
        "    if(self.hidden_layer==0):\n",
        "      # input --> output\n",
        "      f2 = np.dot(f1, self.weights[0]) # N x C\n",
        "      f3 = softmax(f2) # N x C\n",
        "      self.f_params = [f1, f2, f3]\n",
        "      return f3\n",
        "\n",
        "    elif (self.hidden_layer==1):\n",
        "      # input --> first layer\n",
        "      f2 = np.dot(f1, self.weights[0]) # N x M\n",
        "      f3 = self.activation_function(f2) # N x M\n",
        "\n",
        "      # first layer --> output\n",
        "      f4 = np.dot(f3, self.weights[1]) # N x C\n",
        "      f5 = softmax(f4) # N x C\n",
        "      # print(\"f1\", f1[:3])\n",
        "      # print(\"f2\", f2[:3])\n",
        "      # print(\"f3\", f3[:3])\n",
        "      # print(\"f4\", f4[:3])\n",
        "      # print(\"f5\", f5[:3])\n",
        "      self.f_params = [f1, f2, f3, f4, f5]\n",
        "      return f5\n",
        "\n",
        "    elif (self.hidden_layer==2):\n",
        "      # input --> first layer\n",
        "      f2 = np.dot(f1, self.weights[0]) # N x M\n",
        "      f3 = self.activation_function(f2) # N x M\n",
        "\n",
        "      # first layer --> second layer\n",
        "      f4 = np.dot(f3, self.weights[1]) # N x M\n",
        "      f5 = self.activation_function(f4) # N x M\n",
        "\n",
        "      # second layer --> output\n",
        "      f6 = np.dot(f5, self.weights[2]) # N x C\n",
        "      f7 = softmax(f6) # N x C\n",
        "      self.f_params = [f1, f2, f3, f4, f5, f6, f7]\n",
        "      return f7\n",
        "    \n",
        "    else:\n",
        "      print(\"No forward pass\")\n",
        "      return None\n",
        "\n",
        "  \n",
        "  def back_prop(self, y_pred, y_train):\n",
        "    N = y_pred.shape[0]\n",
        "    b1 = y_pred-y_train # N x C\n",
        "    # print('y_pred', y_pred[:3])\n",
        "    # print('p_train', y_train[:3])\n",
        "    if(self.hidden_layer==0):\n",
        "      f1, f2, f3 = self.f_params\n",
        "\n",
        "      e1 = b1 * softmax_diff(f2) # N x C\n",
        "      b2 = np.dot(f1.T, e1)/N # D x C\n",
        "\n",
        "      return [b2]\n",
        "\n",
        "    elif (self.hidden_layer==1):\n",
        "      f1, f2, f3, f4, f5 = self.f_params\n",
        "      # print(\"b1\", b1[:3])\n",
        "      # print(\"f4\", f4[:3])\n",
        "      e1 = b1 * softmax_diff(f4) # N x C\n",
        "      b2 = np.dot(f3.T, e1)/N # M x C\n",
        "      # print(\"e1\", e1[:3])\n",
        "      # print(\"f3\", f3[:3])\n",
        "      # print(\"b2\", b2[:3])\n",
        "      b3 = np.dot(e1, self.weights[1].T) # N x M\n",
        "\n",
        "      e2 = self.activation_function(f2, derivative=True)\n",
        "      b4 = np.dot(f1.T, b3*e2)/N # D x M\n",
        "      # print(\"b4\", b4[:3])\n",
        "      # print(\"b2\", b2[:3])\n",
        "      return [b4, b2]\n",
        "\n",
        "    elif (self.hidden_layer==2):\n",
        "      f1, f2, f3, f4, f5, f6, f7 = self.f_params\n",
        "\n",
        "      e1 = b1 * softmax_diff(f6) # N x C\n",
        "      b2 = np.dot(f5.T, e1)/N # M x C\n",
        "\n",
        "      b3 = np.dot(e1, self.weights[2].T) # N x M\n",
        "\n",
        "      e2 = self.activation_function(f4, derivative=True) # N x M\n",
        "      b4 = np.dot((b3*e2).T, f3)/N # M x M\n",
        "\n",
        "      b5 = np.dot(b3*e2, self.weights[1].T) # N x M\n",
        "\n",
        "      e3 = self.activation_function(f2, derivative=True) # N x M\n",
        "      b6 = np.dot(f1.T, (b5*e3))/N # D x M\n",
        "\n",
        "      return [b6, b4, b2]\n",
        "    \n",
        "    else:\n",
        "      print(\"No backpropagation\")\n",
        "      return None\n",
        "  \n",
        "  def update_weights(self, dweights):\n",
        "    self.weights -= self.learning_rate * dweights\n",
        "\n",
        "  def predict(self, x_test):\n",
        "    yh = self.forward_pass(x_test)\n",
        "    return np.argmax(yh, axis=-1)\n",
        "\n",
        "  def eval_acc(self, y_pred, y_test):\n",
        "    acc = y_pred==y_test\n",
        "    # print(\"Prediction: 0 --> %d ; 1 --> %d ; 2 --> %d ; 3 --> %d ; 4 --> %d ; 5 --> %d ; 6 --> %d ; 7 --> %d ; 8 --> %d ; 9 --> %d\"  % (np.sum(y_pred == 0), np.sum(y_pred==1), np.sum(y_pred==2), np.sum(y_pred==3), np.sum(y_pred==4), np.sum(y_pred==5), np.sum(y_pred==6), np.sum(y_pred==7), np.sum(y_pred==8), np.sum(y_pred==9)))\n",
        "    return np.sum(acc)/len(y_test)\n",
        "    # return(np.mean((y_pred==y_test)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validation and kFold Validation Testing"
      ],
      "metadata": {
        "id": "-7KDitu1Ou8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(x_train, y_train, L, i):\n",
        "  block = int(1/L * x_train.shape[0])\n",
        "  val_mask = np.zeros(x_train.shape[0], dtype=bool)\n",
        "  start = block*i\n",
        "  end = block*(i+1)\n",
        "  val_mask[start:end] = True # select validation set only\n",
        "  x_tr = x_train[val_mask]\n",
        "  y_tr = y_train[val_mask]\n",
        "  x_val = x_train[val_mask==False] # invert indices boolean to get train set\n",
        "  y_val = y_train[val_mask==False] # invert indices boolean to get train set\n",
        "  return x_tr, y_tr, x_val, y_val"
      ],
      "metadata": {
        "id": "Re-I47SjOug7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kFoldCV(model, x, y, L):\n",
        "  train_acc, val_acc = [], []\n",
        "  for k in range(L):\n",
        "    x_train, y_train, x_val, y_val = cross_validation(x, y, L, k)\n",
        "    model.fit(x_train, y_train, 32)\n",
        "    y_val_pred = model.predict(x_val)\n",
        "    y_train_pred = model.predict(x_train)\n",
        "    train_acc.append(model.eval_acc(y_train_pred, y_train))\n",
        "    val_acc.append(model.eval_acc(y_val_pred, y_val))\n",
        "    print(\"%d/%d fold completed\" %(k+1,L))\n",
        "  print(\"Mean [training, validation] accuracy of [%.3f, %.3f]\" % \n",
        "        (np.mean(train_acc), np.mean(val_acc)))\n",
        "  return train_acc, val_acc\n"
      ],
      "metadata": {
        "id": "ld-Rq0nQVhnZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VItK1k56dvsZ"
      },
      "source": [
        "## Activation Function\n",
        "Sigmoid, ReLu, Leaky ReLu, tanh for intermediate layer \n",
        "\n",
        "Softmax and derivative for classification and gradient update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "F-oYE_F6cu4E"
      },
      "outputs": [],
      "source": [
        "sigmoid = lambda z: 1./ (1 + np.exp(-z))\n",
        "\n",
        "relu = lambda z: np.maximum(0, z)\n",
        "\n",
        "tanh = lambda z: np.tanh(z)\n",
        "\n",
        "leaky_relu = lambda z: np.maximum(0.1*z, z)\n",
        "\n",
        "# prevent overflow in exponential using z.max()\n",
        "softmax = lambda z: np.exp(z-z.max()) / np.sum(np.exp(z-z.max()), axis = -1)[:, None] \n",
        "softmax_diff = lambda z: softmax(z)*(1-softmax(z))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N4SfaBsfX4C"
      },
      "source": [
        "# Main Body of Code\n",
        "## Importing Data and normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwHaJAg0Iwp4",
        "outputId": "7f43d12e-0c4e-4d19-ca8a-bcedf0690e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train image shape:  (60000, 28, 28)\n",
            "Train label shape:  (60000,)\n",
            "\n",
            "Test image shape:  (10000, 28, 28)\n",
            "Test label shape:  (10000,)\n"
          ]
        }
      ],
      "source": [
        "mnist_dataset = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
        "\n",
        "print(\"Train image shape: \", train_images.shape)\n",
        "print(\"Train label shape: \", train_labels.shape)\n",
        "print(\"\\nTest image shape: \", test_images.shape)\n",
        "print(\"Test label shape: \", test_labels.shape)\n",
        "\n",
        "# normalize pixel values to domain of 0 and 1\n",
        "x_train = train_images/255\n",
        "y_train = train_labels\n",
        "\n",
        "x_test = test_images/255\n",
        "y_test = test_labels\n",
        "\n",
        "#vectorize the images\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "fraction = 0.10 # 10% validation and 90% training set\n",
        "\n",
        "x_train = x_train[:int(x_train.shape[0]*fraction)]\n",
        "y_train = y_train[:int(y_train.shape[0]*fraction)]\n",
        "\n",
        "x_test = x_test[:int(x_test.shape[0]*fraction)]\n",
        "y_test = y_test[:int(y_test.shape[0]*fraction)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "SECVORJh-VnZ",
        "outputId": "61e5b8be-88a4-430f-a1fc-69bb221a389c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/5 fold completed\n",
            "2/5 fold completed\n",
            "3/5 fold completed\n",
            "4/5 fold completed\n",
            "5/5 fold completed\n",
            "Mean [training, validation] accuracy of [0.985, 0.835]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9883333333333333, 0.9766666666666667, 0.9833333333333333, 0.99, 0.9866666666666667]\n",
            "[0.7877083333333333, 0.8289583333333334, 0.8375, 0.8485416666666666, 0.8733333333333333]\n"
          ]
        }
      ],
      "source": [
        "model = MLP(x_test.shape[1], np.max(y_test)+1, activation=\"leaky-relu\", hidden_layer=1)\n",
        "x_train_acc, x_val_acc = kFoldCV(model, x_train, y_train, 5)\n",
        "plt.plot(np.arange(5)+1, x_train_acc, linewidth=2, color='green', label=\"Training\")\n",
        "plt.plot(np.arange(5)+1, x_val_acc, linewidth=2, color='blue', label=\"Validation\")\n",
        "plt.xlabel(\"kFold\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n",
        "print(x_train_acc)\n",
        "print(x_val_acc)\n",
        "# mlp.fit(x_train, y_train)\n",
        "# yh = mlp.predict(x_test)\n",
        "# print(\"\")\n",
        "# print(\"Accuracy of validation data: \", 100*mlp.eval_acc(yh, y_test), \"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST AREA"
      ],
      "metadata": {
        "id": "GDIW7NXRT7oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dweights = [[2,3], [1,2], [1,1]]\n",
        "# print(len(dweights))\n",
        "# dweights = np.array([1,1])\n",
        "# norm = np.linalg.norm(dweights) if dweights.ndim==1 else [np.linalg.norm(g) for g in dweights]\n",
        "# norm = [np.linalg.norm(g) for g in array[2]]\n",
        "# print(norm)\n",
        "# print(\"%\")\n",
        "xtick = np.arange(5)+1\n",
        "print(xtick)"
      ],
      "metadata": {
        "id": "PlBvNR8dT7EN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VItK1k56dvsZ"
      ],
      "name": "Group 18 - Miniproject 3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZTCTBH1DBAO5TIKG+E3d3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}